{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as ptch\n",
    "import matplotlib.colors as plcl\n",
    "import skimage.measure as measure\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem zadania była analiza dwóch obrazów przedstawiających różne grupy obiektów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, dpi = 20, log_scale = False, title = \"\", cmap = \"gray\"):\n",
    "    if log_scale:\n",
    "        image = np.log(image)\n",
    "\n",
    "    width, height = image.shape\n",
    "    fig = plt.figure(figsize = (width / dpi, height / dpi))\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.set_title(title)\n",
    "    ax.imshow(image, cmap = cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(image_arr, pattern_arr):\n",
    "    pattern_arr_rot = np.rot90(pattern_arr, 2)\n",
    "    pattern_fft = np.fft.fft2(pattern_arr_rot, image_arr.shape)\n",
    "    image_fft = np.fft.fft2(image_arr)\n",
    "    return np.real(np.fft.ifft2(np.multiply(pattern_fft, image_fft)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_correlation(corr_arr, rate):\n",
    "    filtered = np.copy(corr_arr)\n",
    "    filtered[filtered <= np.max(filtered) * rate] = 0\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detected_pattern_text(image_arr, corr_arr, pattern_shape, dpi = 20):\n",
    "    width, height = image_arr.shape\n",
    "    fig = plt.figure(figsize = (width / dpi, height / dpi))\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.imshow(image_arr, cmap = \"gray\")\n",
    "    \n",
    "    for y in range(corr_arr.shape[0]):\n",
    "        for x in range(corr_arr.shape[1]):\n",
    "            if corr_arr[y][x] != 0:\n",
    "                mark = ptch.Rectangle((x, y), -pattern_shape[1], -pattern_shape[0], linewidth=1, edgecolor=\"y\", facecolor=\"none\")\n",
    "                ax.add_patch(mark)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_detected_pattern_text(corr_arr):\n",
    "    return np.count_nonzero(corr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detected_pattern_image(image_arr, corr_arr, dpi = 20):\n",
    "    width, height = image_arr.shape\n",
    "    fig = plt.figure(figsize = (width / dpi, height / dpi))\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.imshow(image_arr, cmap = \"gray\")\n",
    "    \n",
    "    corr_arr[corr_arr == 0] = np.nan\n",
    "    ax.imshow(corr_arr, interpolation = 'none', cmap = 'Reds')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_detected_pattern_image(corr_arr, rate):\n",
    "    return len(measure.find_contours(corr_arr, rate * np.max(corr_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dft_on_image(image_path, pattern_path, rate = 0.9, dpi = 20, invert = True, text_image = True):\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    pattern = Image.open(pattern_path).convert(\"L\")\n",
    "    \n",
    "    if invert:\n",
    "        image = PIL.ImageOps.invert(image)\n",
    "        pattern = PIL.ImageOps.invert(pattern)\n",
    "\n",
    "    image_arr = np.asarray(image)\n",
    "    pattern_arr = np.asarray(pattern)\n",
    "\n",
    "    plot_image(image_arr, dpi, title = \"Converted image\")\n",
    "    \n",
    "    corr_arr = correlation(image_arr, pattern_arr)\n",
    "    plot_image(corr_arr, dpi, title = \"Correlation\")\n",
    "\n",
    "    filtered_corr = filter_correlation(corr_arr, rate)\n",
    "    plot_image(filtered_corr, dpi, title = \"Filtered correlation\")\n",
    "    \n",
    "    if text_image:\n",
    "        print(f\"Detected patterns count {count_detected_pattern_text(filtered_corr)}\")\n",
    "        show_detected_pattern_text(image_arr, filtered_corr, pattern_arr.shape, dpi)\n",
    "    else:\n",
    "        print(f\"Detected patterns count {count_detected_pattern_image(filtered_corr, rate)}\")\n",
    "        show_detected_pattern_image(image_arr, filtered_corr, dpi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do obliczenia korelacji między wzorcem (obróconym o 180 stopni) i całym obrazem skorzystano z transformaty Fouriera. Następnie przefiltrowano macierz korelacji, eliminując wartości niższe niż zadany parametrem \"rate\" próg odcięcia pomnożony przez maksymalną wartość z macierzy korelacji. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza obrazu przedstawiającego tekst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku obrazu przedstawiającego tekst, przekonwertowano go do odcieni szarości oraz odwrócono kolory (tak samo ze wzorcem). Następnie obliczono korelację oraz przefiltrowano otrzymaną macierz korelacji. Obliczono ilość wystąpień wzorca jako ilość niezerowych elementów w macierzy korelacji. Zwizualizowano miejsca w których wykryto wzorzec przez oznaczenie ich żółtą ramką bezpośrednio na wejściowym zdjęciu. Program bezbłędnie zidentyfikował wszystkie wystąpienia litery e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-43da35f5136c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompute_dft_on_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pattern_text.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.94\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-eb765d538a93>\u001b[0m in \u001b[0;36mcompute_dft_on_image\u001b[1;34m(image_path, pattern_path, rate, dpi, invert, text_image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_dft_on_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anian\\anaconda3\\envs\\mownit_lab\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2843\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text.png'"
     ]
    }
   ],
   "source": [
    "compute_dft_on_image(\"text.png\", \"pattern_text.png\", 0.94, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d48855b0b786>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfftshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplot_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Wykres modułów współczynników Fouriera\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anian\\anaconda3\\envs\\mownit_lab\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2843\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text.png'"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"text.png\").convert(\"L\")\n",
    "image_arr = np.asarray(image)\n",
    "image_freq = np.fft.fftshift(np.fft.fft2(image_arr))\n",
    "modules = np.absolute(image_freq)\n",
    "plot_image(modules, log_scale = True, dpi = 60, title = \"Wykres modułów współczynników Fouriera\")\n",
    "phases = np.angle(image_freq)\n",
    "plot_image(phases, dpi = 60, title = \"Wykres faz współczynników Fouriera\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza obrazu przedstawiającego ławicę ryb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku obrazu przedstawiającego ławicę ryb, nie dokonywano odwrócenia kolorów, jedynie przekonwertowano go do odcieni szarości. Obliczono i przefiltrowano korelację, a następnie zwizualizowano wystąpienia wzorca. Do obliczenia ilości wystąpień skorzystano z funkcji find_contours z pakietu skimage.measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dft_on_image(\"fish.jpg\", \"pattern_fish.png\", 0.66, 100, invert = False, text_image = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fish.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ed4e8fc8b393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fish.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfftshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplot_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Wykres modułów współczynników Fouriera\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anian\\anaconda3\\envs\\mownit_lab\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2843\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fish.jpg'"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"fish.jpg\").convert(\"L\")\n",
    "image_arr = np.asarray(image)\n",
    "image_freq = np.fft.fftshift(np.fft.fft2(image_arr))\n",
    "modules = np.absolute(image_freq)\n",
    "plot_image(modules, log_scale = True, dpi = 60, title = \"Wykres modułów współczynników Fouriera\")\n",
    "phases = np.angle(image_freq)\n",
    "plot_image(phases, dpi = 60, title = \"Wykres faz współczynników Fouriera\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie polegało na napisaniu programu przekształcającego obraz w tekst. Program wykrywa małe litery alfabetu angielskiego, cyfry, białe znaki (spację i przejście do nowej linii) oraz następujące znaki interpunkcyjne: .,!?. Program wykrywa czcionkę szeryfową i bezszeryfową. Dla każdej z czcionek został przygotowany wzorzec. Przygotowano także kilka testowych obrazów uwzględniających przypadki występowania szumu oraz obrócenia całości tekstu. Logika programu jest zaimplementowana w dwóch klasach - FontLoader i OCRRecognizer. Podobnie jak w poprzednim zadaniu, do wykrywania danego wzorca wykorzystano operację splotu i transformatę Fouriera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontLoader:\n",
    "    chars = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"t\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"dot\", \"comma\", \"question-mark\", \"exclamation-mark\"]\n",
    "    images = {}\n",
    "\n",
    "    def __init__(self, font_type):\n",
    "        for char in self.chars:\n",
    "            char_image = Image.open(f\"{font_type}/{char}.png\").convert(\"L\")\n",
    "            self.images[char] = PIL.ImageOps.invert(char_image)\n",
    "            \n",
    "    def get_char_img(self, char):\n",
    "        return np.asarray(self.images[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRRecognizer:\n",
    "    def __init__(self, image_path, font_type, original_text = None):\n",
    "        self.image = Image.open(image_path).convert(\"L\")\n",
    "        self.image = PIL.ImageOps.invert(self.image)\n",
    "        self.image_arr = np.array(self.image)\n",
    "        self.font = FontLoader(font_type)\n",
    "        self.char_positions = []\n",
    "        self.original_text = original_text\n",
    "        self.font_type = font_type\n",
    "        self.special_chars = {\n",
    "            \"exclamation-mark\": \"!\", \n",
    "            \"question-mark\": \"?\", \n",
    "            \"comma\": \",\", \n",
    "            \"dot\": \".\"\n",
    "        }\n",
    "\n",
    "        \n",
    "    def rotate_image(self):\n",
    "        thresh = cv2.threshold(self.image_arr, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "        coords = np.column_stack(np.where(thresh > 0))\n",
    "        angle = cv2.minAreaRect(coords)[-1]\n",
    "\n",
    "        if angle < -45:\n",
    "            angle = -(90 + angle)\n",
    "        else:\n",
    "            angle = -angle\n",
    "\n",
    "        (h, w) = self.image_arr.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        self.image_arr = cv2.warpAffine(self.image_arr, M, (w, h),flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "        \n",
    "    def denoise_image(self, rate = 0.3):\n",
    "        img_fft = np.fft.fft2(self.image_arr)\n",
    "        img_fft[int(width * rate):int(width * (1 - rate))] = 0\n",
    "        img_fft[:, int(height * rate):int(height * (1 - rate))] = 0\n",
    "\n",
    "        self.image_arr = np.abs(np.fft.ifft2(img_fft).real)\n",
    "\n",
    "                \n",
    "    def plot_image(self, dpi=40.0):\n",
    "        width, height = self.image_arr.shape\n",
    "        fig = plt.figure(figsize=(width / dpi, height / dpi))\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        ax.imshow(self.image_arr, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def show_detected_pattern_text(self, corr_arr, char, dpi = 40.0):\n",
    "        width, height = self.image_arr.shape\n",
    "        fig = plt.figure(figsize = (width / dpi, height / dpi))\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        ax.imshow(self.image_arr, cmap = \"gray\")\n",
    "        \n",
    "        char_img_shape = self.font.get_char_img(char).shape\n",
    "        \n",
    "        for y in range(corr_arr.shape[0]):\n",
    "            for x in range(corr_arr.shape[1]):\n",
    "                if corr_arr[y][x] != 0:\n",
    "                    mark = ptch.Rectangle((x, y), -char_img_shape[1], -char_img_shape[0], linewidth=1, edgecolor=\"y\", facecolor=\"none\")\n",
    "                    ax.add_patch(mark)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def remove_char(self, corr_arr, char):\n",
    "        char_img_shape = self.font.get_char_img(char).shape\n",
    "\n",
    "        for y in range(corr_arr.shape[0]):\n",
    "            for x in range(corr_arr.shape[1]):\n",
    "                if corr_arr[y][x] != 0:\n",
    "                    y_upp = y - char_img_shape[0]\n",
    "                    x_upp = x - char_img_shape[1]\n",
    "                    self.char_positions.append((y_upp, x_upp, char))\n",
    "                    self.image_arr[y_upp : y + 1, x_upp : x + 1] = 0\n",
    "                    \n",
    "        \n",
    "    def filter_correlation(self, corr_arr, rate):\n",
    "        filtered_corr = np.copy(corr_arr)\n",
    "        filtered_corr[corr_arr <= np.max(corr_arr) * rate] = 0\n",
    "        return filtered_corr\n",
    "    \n",
    "        \n",
    "    def compute_correlation(self, char):\n",
    "        char_arr = self.font.get_char_img(char)\n",
    "        char_arr_rot = np.rot90(char_arr, 2)\n",
    "        char_fft = np.fft.fft2(char_arr_rot, self.image_arr.shape)\n",
    "        image_fft = np.fft.fft2(self.image_arr)\n",
    "        return np.real(np.fft.ifft2(np.multiply(char_fft, image_fft)))\n",
    "    \n",
    "    \n",
    "    def match_char(self, char, rate, show_step):\n",
    "        corr_arr = self.compute_correlation(char)\n",
    "        filtered_corr = self.filter_correlation(corr_arr, rate)\n",
    "        if show_step:\n",
    "            self.show_detected_pattern_text(filtered_corr, char)\n",
    "        self.remove_char(filtered_corr, char)\n",
    "        \n",
    "        \n",
    "    def match_chars(self, show_steps = False):\n",
    "        \n",
    "        if self.font_type == \"sans-serif\":\n",
    "            chars_order = [\"a\", \"b\", \"d\", \"e\", \"f\", \"g\", \"p\", \"q\", \"4\", \"t\", \"w\", \"k\", \"x\", \"y\", \n",
    "                           \"v\", \"z\", \"s\", \"1\", \"2\", \"8\", \"3\", \"5\", \"6\", \"7\", \"9\", \"0\", \"u\", \"h\", \n",
    "                           \"j\", \"l\", \"m\", \"n\", \"o\", \"c\", \"r\", \"i\", \"exclamation-mark\", \"question-mark\", \"comma\", \"dot\"]\n",
    "        else:\n",
    "            chars_order = [\"a\", \"d\", \"e\", \"g\", \"p\", \"q\", \"w\", \"k\", \"x\", \"y\", \"v\", \"z\", \"s\", \"4\", \n",
    "                           \"2\", \"8\", \"6\", \"9\", \"3\", \"5\", \"7\", \"b\", \"h\", \"f\", \"j\", \"1\", \"l\", \"m\", \"n\", \n",
    "                           \"u\", \"r\", \"t\", \"0\", \"o\", \"c\", \"i\", \"exclamation-mark\", \"question-mark\", \"comma\", \"dot\"]\n",
    "\n",
    "\n",
    "        if self.font_type == \"sans-serif\":\n",
    "            custom_rates = {\n",
    "                \"r\": 0.87, \"u\": 0.89, \"1\": 0.95, \"5\": 0.95, \"j\": 0.96, \"l\": 0.9, \"i\": 0.85, \"0\": 0.97, \"v\": 0.85, \"exclamation-mark\": 0.95, \"dot\": 0.85, \"3\": 0.95, \"9\": 0.95\n",
    "            }\n",
    "        else:\n",
    "            custom_rates = {\n",
    "                \"1\": 0.98, \"n\": 0.92, \"6\": 0.92, \"b\": 0.95, \"h\": 0.93, \"0\": 0.97, \"u\": 0.94, \"f\": 0.94, \"t\": 0.93, \"j\": 0.9, \"l\": 0.92, \"i\": 0.9, \"0\": 0.97, \"v\": 0.85, \n",
    "                \"dot\": 0.85, \"exclamation-mark\": 0.97\n",
    "            }\n",
    "\n",
    "\n",
    "        for char in chars_order:\n",
    "#             print(char)\n",
    "            if char in custom_rates:\n",
    "                ocr.match_char(char, custom_rates[char], show_steps)\n",
    "            else:\n",
    "                ocr.match_char(char, 0.9, show_steps)\n",
    "\n",
    "                \n",
    "    def report_detection(self):\n",
    "        print(\"Char Original Converted Percentage\")\n",
    "        for char in self.font.chars:\n",
    "            ch = char if char not in self.special_chars else self.special_chars[char]\n",
    "            orignal_count = self.original_text.lower().count(ch)\n",
    "            converted_count = self.convert_to_text().count(ch)\n",
    "            print(f\"{ch}\\t{orignal_count}\\t{converted_count}\\t{converted_count * 100 // orignal_count}%\")\n",
    "        \n",
    "    \n",
    "    def convert_to_text(self, offset = 8):\n",
    "        self.char_positions.sort()\n",
    "        lines = [[]]\n",
    "        line_no = 0\n",
    "        for i in range(len(self.char_positions) - 1):\n",
    "            y_pos, x_pos, char = self.char_positions[i]\n",
    "            if self.char_positions[i + 1][0] - y_pos < offset or len(lines[-1]) == 0:\n",
    "                lines[line_no].append((x_pos, char))\n",
    "            else:\n",
    "                lines.append([])\n",
    "                line_no += 1\n",
    "        \n",
    "        self.lines = lines\n",
    "        \n",
    "        text = \"\"\n",
    "        space_width = 5\n",
    "        tolerance = 2\n",
    "        \n",
    "\n",
    "        for line in self.lines:\n",
    "            line.sort()\n",
    "\n",
    "            last_x_pos = -1\n",
    "            last_char = None\n",
    "            last_char_width = 0\n",
    "\n",
    "            for x_pos, char in line:\n",
    "                if last_x_pos < 0 or last_x_pos + last_char_width - tolerance <= x_pos or last_char != char:\n",
    "                    if last_x_pos + last_char_width + space_width <= x_pos:\n",
    "                        text += \" \"\n",
    "                    \n",
    "                    if char in self.special_chars:\n",
    "                        text += self.special_chars[char]\n",
    "                    else:\n",
    "                        text += char\n",
    "\n",
    "                    last_x_pos = x_pos\n",
    "                    last_char = char\n",
    "                    last_char_width = self.font.get_char_img(char).shape[1]\n",
    "\n",
    "            text += \"\\n\"\n",
    "\n",
    "\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwersja obrazu z czcionką bezszeryfową\n",
    "Przy konwersji obrazu przyjęto, że po dopasowaniu danej litery wszystkie jej wystąpienia są usuwane z obrazu. Dodatkowo, dla liter określono kolejność przetwarzania. Wynika to z faktu że niektóre litery \"zawierają się\" w innych (np 'c' w 'o' albo 'n' w 'm'). Dodatkowo dla niektórych liter trzeba było zmniejszyć lub zwiększyć poziom odcięcia korelacji. Znalezione wystąpienia wzorców są przechowywane jako krotki (y, x, znak), gdzie (x, y) to współrzędne lewego górnego punktu obszaru w którym wykryto wzorzec. Następnie na tej podstawie podzielono tekst na linie - posortowano te krotki po y, a następnie stwierdzono że jeśli dwie kolejne krotki różnią się wartością y o więcej od zadanego parametru \"offset\" to jest to już następna linia. W każdej z linii posortowano krotki po x. Dla każdego znaku po kolei sprawdzano jaką ma szerokość i brano następny znak oddalony przynajmniej o tyle ile dany znak ma szerokości (co wyeliminowało problem kilku wykryć wzorca w tym samym miejscu). Jeśli kolejny znak był oddalony o większą odległość niż parametr \"space_width\" to wstawiano spację. Jednym z ograniczeń wykorzystanej metody był fakt, że na zdjęciu zawsze musiały znaleźć się wszystkie rozpoznawane znaki - inaczej największa korelacja wystąpiła by w miejscu w którym rzeczywiście nie ma danego znaku, co pogorszyłoby jakość konwersji.\n",
    "\n",
    "Uzyskano dobrą dokładność konwersji. Zdarzają się pewne przekłamania, są to jednak pojedyncze przypadki i dotyczą bardziej \"problematycznych\" liter.\n",
    "\n",
    "Do testowych obrazów przygotowano oryginalne teksty dla porównania ilości wystąpień danego znaku w tekście oryginalnym i odczytanym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leprous = Path(\"leprous.txt\").read_text()\n",
    "\n",
    "ihsahn = Path(\"./ihsahn.txt\").read_text()\n",
    "\n",
    "noise = Path(\"./noise.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1efed2f69ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mocr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOCRRecognizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"insomnium.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sans-serif\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch_chars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Converted text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'null' is not defined"
     ]
    }
   ],
   "source": [
    "ocr = OCRRecognizer(\"insomnium.png\", \"sans-serif\", 0)\n",
    "ocr.plot_image()\n",
    "ocr.match_chars(False)\n",
    "ocr.plot_image()\n",
    "print(\"Converted text\")\n",
    "print(ocr.convert_to_text())\n",
    "# ocr.report_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr = OCRRecognizer(\"ocr_ihsahn.png\", \"sans-serif\", ihsahn)\n",
    "ocr.plot_image()\n",
    "ocr.match_chars()\n",
    "ocr.plot_image()\n",
    "ocr.report_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwersja obróconego obrazu\n",
    "W przypadku obróconego obrazu, przed dopasowywaniem wzorców, przywrócono go do poziomej orientacji. Wykorzystano do tego funkcje biblioteki OpenCV, a całość opiera się na znalezieniu prostokąta zawierającego całość tekstu, określeniu jego nachylenia i obróceniu obrazu o ten kąt. Stopień wykrycia znaków nie uległ pogorszeniu w wyniku tej operacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr = OCRRecognizer(\"ocr_leprous_rotated.png\", \"sans-serif\", leprous)\n",
    "ocr.plot_image()\n",
    "ocr.rotate_image()\n",
    "ocr.plot_image()\n",
    "ocr.match_chars(False)\n",
    "ocr.plot_image()\n",
    "print(ocr.convert_to_text())\n",
    "ocr.report_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwersja obrazu z czcionką szeryfową"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku czcionki szeryfowej konieczna była modyfikacja parametrów - kolejności przetwarzania znaków oraz progów odcięcia. Stopień poprawnego wykrycia znaków uległ nieznacznemu pogorszeniu w stosunku do czcionki bezszeryfowej, aczkolwiek wciąż mieści się w akceptowalnym przedziale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr = OCRRecognizer(\"ocr_ihsahn_serif.png\", \"serif\", ihsahn)\n",
    "ocr.plot_image()\n",
    "ocr.match_chars(False)\n",
    "ocr.plot_image()\n",
    "print(ocr.convert_to_text())\n",
    "ocr.report_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwersja obrazu na którym występuje szum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do usuwania szumu wykorzystano transformatę Fouriera. Po zdekomponowaniu obrazu do dziedziny częstotliwości odcięto wyższe jej wartości. W wyniku uzyskano obraz nieco bardziej rozmyty, natomiast zachowano wysoki procent poprawnie wykrywanych znaków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr = OCRRecognizer(\"ocr_noise.png\", \"sans-serif\", noise)\n",
    "ocr.plot_image()\n",
    "ocr.denoise_image()\n",
    "ocr.plot_image()\n",
    "ocr.match_chars(False)\n",
    "print(ocr.convert_to_text())\n",
    "ocr.report_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski\n",
    "Zastosowana metoda nie jest najlepszą do realizacji OCR. Każda z czcionek wymagała indywidualnego, drobiazgowego doboru parametrów, ponadto nawet niewielkie zakłócenia powodowały duże błędy przy detekcji. Ponadto gdy na testowych obrazach nie występowały wszystkie wykrywane litery to algorytm nie działał poprawnie, co wynikało z doboru maksimów korelacji."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
